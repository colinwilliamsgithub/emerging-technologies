{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e102be2-079b-48c9-a44c-c43644d0ad79",
   "metadata": {},
   "source": [
    "<h1>Task 1: Third-order letter approximation model</h1>\n",
    "\n",
    "For Task 1, I am tasked to select five free English works from Project Gutenberg. I then have to preprocess these texts by removing any preamble and postamble, and removing all characters except for ASCII letters, full stops, and spaces. Finally, I then have to make all letters uppercase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5fa431-bbde-48e9-ba1b-ef8e984f8f6e",
   "metadata": {},
   "source": [
    "When I have preprocessed all the texts, I then have to create a trigram model by counting the number of times each trigram (a sequence of three characters) appears. For example, \"It is what it is.\" would become \"IT IS WHAT IT IS\" when processed. This will then give a trigram model like {'IT ': 2, 'T I': 3, ' IS': 2, 'IS ': 1, 'S W': 1, ' WH': 1, 'WHA': 1, 'HAT': 1, 'AT ': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46d72bd2-8818-4a11-a44f-f9837ada559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6674c2d7-03a0-40be-8315-2e1ca4625e44",
   "metadata": {},
   "source": [
    "Initialises the directory, inistialises the dictionaries and reads each text file.\n",
    "\n",
    "- The texts dictionary stores the texts.\n",
    "- The trigram_counts dictionary stores trigrams and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "188703a6-bed0-4226-9512-7bf330e02e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the text files\n",
    "directory = \"texts\"\n",
    "\n",
    "texts = {}\n",
    "# Initialize an empty dictionary for storing trigrams and their counts\n",
    "trigram_counts = {}\n",
    "\n",
    "# Load each text file into a dictionary\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "            texts[filename] = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb5a00-238d-480f-a308-0daf2ed19360",
   "metadata": {},
   "source": [
    "<h2>Prepare Text</h2>\n",
    "The first step in preparing the texts is to remove any preamble and postamble. Project Gutenburg texts often have a preamble that ends with \"*** START OF THE PROJECT GUTENBERG EBOOK (EBOOK NAME) ***\", and a postamble that starts with \"*** END OF THE PROJECT GUTENBERG EBOOK (EBOOK NAME) ***\". This can be removed by setting markers, in this case \"*** START\" and \"*** END\". The function then searches for where these markers first occur in the text. If both markers are found, the function extracts the portion of the text between these markers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f684bf-ea02-4532-849f-abf22bc681f8",
   "metadata": {},
   "source": [
    "The next step is to remove all characters except letters, full stops and spaces from the text. To do this, the function uses the re.sub() method. This is used to replace unwanted characters in the text. Any character that is not a letter, a full stop or a space is replaced with an empty string, effectively removing them. The text is now cleaned, leaving only letters, full stops and spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd1757-51e9-4e1c-8b38-43755bd42e06",
   "metadata": {},
   "source": [
    "Finally, I have to convert all the characters to uppercase. This is done by using the .upper() method, which transforms all the letters to uppercase.\n",
    "The .strip() method is also used to remove any leading or trailing whitespace from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "979ac4cf-594f-49a1-965c-18ba1cd7901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text):\n",
    "    # Remove preamble and postamble from the text\n",
    "    preamble = text.find(\"*** START\")\n",
    "    postamble = text.find(\"*** END\")\n",
    "    if preamble != -1 and postamble != -1:\n",
    "        text = text[preamble:postamble]\n",
    "    \n",
    "    # Remove all characters except letters, full stops and spaces\n",
    "    prepared_text = re.sub(r\"[^A-Za-z. ]\", \"\", text)\n",
    "    \n",
    "    return prepared_text.upper().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529ff1a7-b25b-485e-b7d0-e5c6461e90fd",
   "metadata": {},
   "source": [
    "<h2>Generate Trigrams</h2>\n",
    "The next step is to generate the trigrams and count the number of times each trigram appears.\n",
    "\n",
    "- for i in range(len(prepared_text) - 2):<br>\n",
    "First, it iterates through the text to extract all possible trigrams.\n",
    "\n",
    "- trigram = prepared_text[i:i + 3]:<br>\n",
    "Then, we have to slice the text to extract a substring of 3 characters.\n",
    "\n",
    "- if trigram in trigram_counts:\n",
    "    trigram_counts[trigram] += 1\n",
    "else:\n",
    "    trigram_counts[trigram] = 1\n",
    "\n",
    "This checks if the trigram is in trigram_counts. If it is, then increase the count by 1. If it's not, then initialise it's count to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "980d9bd5-e52c-4617-b356-7c855a1bf5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram_model(prepared_text):\n",
    "    # Iterate through the text to extract trigrams\n",
    "    for i in range(len(prepared_text) - 2):\n",
    "        # Extract a trigram\n",
    "        trigram = prepared_text[i:i + 3]\n",
    "        \n",
    "        # Increment the count of the trigram in the dictionary\n",
    "        if trigram in trigram_counts:\n",
    "            trigram_counts[trigram] += 1\n",
    "        else:\n",
    "            trigram_counts[trigram] = 1\n",
    "    \n",
    "    return trigram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f36e61-384c-4483-b56a-71f264d39bc1",
   "metadata": {},
   "source": [
    "Now we have to read all the text files, prepare them and get the trigram counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0dca8a9b-7f65-4f5e-bbe5-e28373c60848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each text file\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        # Prepare the text\n",
    "        prepared_text = prepare_text(content)\n",
    "        \n",
    "        build_trigram_model(prepared_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5597f-f0f9-43cb-8494-13485be98895",
   "metadata": {},
   "source": [
    "The trigram counts are displayed in task1trigrams.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1eb74b42-af11-42cb-8f57-7185a69ed3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"task1trigrams.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for trigram, count in sorted(trigram_counts.items()):\n",
    "        file.write(f\"{trigram}: {count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0057ab-1755-4aa7-a497-5d701f1bb2d1",
   "metadata": {},
   "source": [
    "<h1>Task 2: Third-order letter approximation generation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3fae46-5eec-4ab4-b180-6070048971d0",
   "metadata": {},
   "source": [
    "In this task, I have to use the model from task 1 to make a string with 10,000 characters, starting with \"TH.\" For each new character, look at the last two characters to find matching trigrams in the model. Then, pick the next character randomly based on how often it appears in those trigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c91b1c-db16-40bc-9df3-6dfc0a5478ba",
   "metadata": {},
   "source": [
    "First, I have to select the starting string, which is \"TH\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06590281-8177-41b5-82c6-4aeaa142939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 'TH'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4bba20-1fc0-4b35-b7c6-656e9cd31015",
   "metadata": {},
   "source": [
    "Now I have to make the loop that will find all the trigrams in the trigram_counts dictionary whose first two characters match the last two characters of the \"start\" string, randomly select the next character based on the weights, and append it to the \"start\" string. For referance, I used the notes provided on the emerging-technologies repo: https://github.com/ianmcloughlin/2425_emerging_technologies/blob/main/02_language_models.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1641d62c-1d62-4dba-8685-14a78d88fa4e",
   "metadata": {},
   "source": [
    "<h5>letters, weights = list(zip(*[(x[2], trigram_counts[x]) for x in trigram_counts.keys() if x[:2] == start[-2:]]))</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e2c795-1530-4279-9ce9-172fdae2a84d",
   "metadata": {},
   "source": [
    "- x[:2] == start[-2:]:<br>\n",
    "This will filter trigrams whose first two characters (x[:2]) match the last two characters of start (start[-2:]).\n",
    "- for x in trigram_counts.keys() if x[:2] == start[-2:]]:<br>\n",
    "If they are equal, retrieve all trigrams stored in the trigram_counts dictionary.\n",
    "- (x[2], trigram_counts[x]):<br>\n",
    "(x[2]) extracts the third character of the trigram, and (trigram_counts[x]) retrieves the count of that trigram from the model, which will be used as the weight for weighted random selection.\n",
    "- zip: Separates the third characters into a letters tuple and their corresponding counts into a weights tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f92682-6094-4268-9b3d-ad9e955162e9",
   "metadata": {},
   "source": [
    "<h5>start += random.choices(letters, weights=weights, k=1)[0]</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780bd5e3-8a5e-4ce9-bab2-ee44aff51f6a",
   "metadata": {},
   "source": [
    "In this line,\n",
    "- letters represents the list of possible next characters,\n",
    "- weights represents their corresponding counts, which is used as probabilities,\n",
    "- and k=1 generates one random choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842ebf7-6bb1-4d0e-aa69-04a4d8fff6cd",
   "metadata": {},
   "source": [
    "[0] then extracts the single character that is returned, and it is then appended to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96934c62-c979-4ab2-bda6-d43a303edae1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 9999):\n",
    "    # Select all of the keys that start with the last 2 characters in start.\n",
    "    letters, weights = list(zip(*[(x[2], trigram_counts[x]) for x in trigram_counts.keys() if x[:2] == start[-2:]]))\n",
    "\n",
    "    # Generate the next character.\n",
    "    start += random.choices(letters, weights=weights, k=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b30da2-4dae-4123-8320-41bb8c0c341b",
   "metadata": {},
   "source": [
    "The generated text is displayed in task2randomstring.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c916837f-cf45-45ad-a51c-f1a5cedd331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"task2randomstring.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b4768c-6775-4aa6-9b79-d7950a02c83d",
   "metadata": {},
   "source": [
    "<h1>Task 3: Analyze the model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355eaf3e-09ad-461a-a081-5358f6cf806c",
   "metadata": {},
   "source": [
    "For this task, I have to copy the list of English words available in words.txt from the assignment repository to my own repository. I will then use it to determine the percentage of words in my 10,000 characters that are actual words in the English language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c497e-52d0-4eb0-8a0f-5ddfb0009461",
   "metadata": {},
   "source": [
    "First I have load the list of English words into a set. A set is used here rather than a list because it's faster for checking if a word exists. This is because sets are implemented as hash tables, so checking membership is a constant-time operation: O(1). In contrast, checking membership in a list requires scanning through each element, which is a linear-time operation: O(n)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e5d47b-8a5b-4996-90f6-f5e31dfc56ec",
   "metadata": {},
   "source": [
    "I then prepare the text by using .upper() to convert the words to uppercase to match the format of the text, and use .strip() to remove any extra spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42303514-b7ad-4b36-9215-511fe7f75507",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"words.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    words = set(word.strip().upper() for word in file.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca192505-2989-4814-820e-c268b9cc2b17",
   "metadata": {},
   "source": [
    "Then I split the text into words by using start.split(). This will split the text by spaces, creating a list of words. I also initialise the word count so that it starts from 0 every time it's ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee47e1e9-25ae-4cb4-af50-d01a54158831",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_words = start.split()\n",
    "word_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f618642-057c-4123-880d-b7bdccdcfefa",
   "metadata": {},
   "source": [
    "Now I have to count the number of English words. To do this, I iterate through the words in split_words and check if the word exists in the words set. If it does, increase the count of word_count by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b04b9fe3-7e0a-4b43-888b-d447da357a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in split_words:\n",
    "    if word in words:\n",
    "        word_count = word_count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd19ff3b-8850-4fae-94f9-ebff4d532486",
   "metadata": {},
   "source": [
    "The final step is to calculate the percentage of English words. This is done by dividing the word_count by the total_words, and multiplying by 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9882c949-c143-4109-aba8-acaea97f88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = len(start)\n",
    "percentage = (word_count / total_words) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e628a-7ff7-43fc-bea5-aaaf6ead28e4",
   "metadata": {},
   "source": [
    "Now we can print our results to see the total words, word count, and the percentage of English words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d16a480e-e871-4a9c-a360-a71eedbdd479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 10000\n",
      "English words: 623\n",
      "Percentage of English words: 6.23%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total words: {total_words}\")\n",
    "print(f\"English words: {word_count}\")\n",
    "print(f\"Percentage of English words: {percentage:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
